{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parra_Martin_Ejercicio_1_GPU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR30pBLa8Dzo"
      },
      "source": [
        "**1. Introducción**\n",
        "\n",
        "El siguiente cuaderno realiza la multiplicación de 2 vectores utilizando GPGPU. El algoritmo representa una \"similitud\" con el algoritmo dot[4] con la diferencia que en el ejercicio el resultado obtenido termina siendo un vector y no un escalar.\n",
        "\n",
        "La lógica planteada en dicho ejercicio es la siguiente: \n",
        "\n",
        "                      Y[0] = X[0] * Y[0]\n",
        "                      Y[1] = X[1] * Y[1]\n",
        "                      Y[2] = X[2] * Y[2]\n",
        "                                .\n",
        "                                .\n",
        "                                .\n",
        "                                .\n",
        "                                .\n",
        "                      Y[n] = X[n] * Y[n]\n",
        "\n",
        "\n",
        "Siendo n, la cantidad de elementos ingresados por parámetro.\n",
        "\n",
        "Realizada en lenguaje Python[1], utilizando Google Colab[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PTtp2dZ8HIe"
      },
      "source": [
        "**2. Armado del ambiente**\n",
        "\n",
        "Instala en el cuaderno el módulo CUDA de Python. [3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgJznesH8Ssl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7746926-088b-4698-c64a-1eda06fa7451"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 9.0MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=621228 sha256=b77a242437da0baf46ec8498c5549a12857fa8230b84dccadb09ddc04202ce56\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=593c94b039e8b283dffa9f83fcbf14b15a1177940d646ad0aa656e6fd91a1fe9\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImIWHR7X8KW2"
      },
      "source": [
        "**3. Desarrollo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsyEb-tX8N4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ff071a-ca5b-4faf-f6b8-cc1c7a6c314d"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cantidad_N =   10000#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "try:\n",
        "  if cantidad_N <= 0:\n",
        "    print(\"Debe ingresar un valor númerico mayor a 0\")\n",
        "  else:\n",
        "    # CPU - Defino la memoria de los vectores en cpu.\n",
        "    x_cpu = numpy.random.randn( cantidad_N )\n",
        "    x_cpu = x_cpu.astype( numpy.float32() )\n",
        "\n",
        "    y_cpu = numpy.random.randn( cantidad_N )\n",
        "    y_cpu = y_cpu.astype( numpy.float32() )\n",
        "\n",
        "    #tiempo_ini_cpu = datetime.now()\n",
        "\n",
        "    r_cpu = numpy.empty_like( y_cpu )\n",
        "\n",
        "\n",
        "    # CPU - reservo la memoria GPU.\n",
        "    x_gpu = cuda.mem_alloc( x_cpu.nbytes )\n",
        "    y_gpu = cuda.mem_alloc( y_cpu.nbytes )\n",
        "\n",
        "    # GPU - Copio la memoria al GPU.\n",
        "    cuda.memcpy_htod( x_gpu, x_cpu )\n",
        "    cuda.memcpy_htod( y_gpu, y_cpu )\n",
        "\n",
        "    # CPU - Defino la función kernel que ejecutará en GPU.\n",
        "    module = SourceModule(\"\"\"\n",
        "    __global__ void kernel_prodVectorial( int n, float *X, float *Y)\n",
        "    {\n",
        "      int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "      if (idx < n)\n",
        "      {\n",
        "          Y[idx]  = X[idx] * Y[idx];\n",
        "      }\n",
        "\n",
        "    }\n",
        "    \"\"\") \n",
        "    # CPU - Genero la función kernel.\n",
        "    kernel = module.get_function(\"kernel_prodVectorial\")\n",
        "\n",
        "    tiempo_gpu = datetime.now()\n",
        "\n",
        "    \n",
        "    dim_hilo = 256\n",
        "    dim_bloque = numpy.int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "    \n",
        "    # GPU - Ejecuta el kernel.\n",
        "    kernel( numpy.int32(cantidad_N), x_gpu, y_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "    tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "    # GPU - Copio el resultado desde la memoria GPU.\n",
        "    cuda.memcpy_dtoh( r_cpu, y_gpu )\n",
        "\n",
        "    tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "\n",
        "    # CPU - Informo el resutlado.\n",
        "    print( \"------------------------------------\")\n",
        "    print( \"Vector Resultante: \" )\n",
        "    print( r_cpu )\n",
        "    print( \"------------------------------------\")\n",
        "\n",
        "    print( \"Cantidad de elementos: \", cantidad_N )\n",
        "    print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "    print(\"Tiempo Total: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "    print(\"Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )\n",
        "except Exception as exception:\n",
        "  print(\"Ha ocurrido una excepcion: \", exception)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Vector Resultante: \n",
            "[ 0.49734148 -0.01866157  0.13272288 ...  0.6960769  -0.36827222\n",
            "  0.3486228 ]\n",
            "------------------------------------\n",
            "Cantidad de elementos:  10000\n",
            "Thread x:  256 , Bloque x: 40\n",
            "Tiempo Total:  2.794 [ms]\n",
            "Tiempo GPU:  0.077 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PV9KXqncC6W"
      },
      "source": [
        "**4. Tabla de pasos**\n",
        "\n",
        "Procesador |\tFunción |\tDetalle\n",
        "-----------|----------|--------\n",
        "CPU\t       | @param\t  | Lectura del tamaño de vectores desde Colab.\n",
        "CPU\t       | import\t  | Importa los módulos para funcionar.\n",
        "CPU\t       | datetime.now()\t| Toma el tiempo actual.\n",
        "CPU        | if            | Verifico la cantidad ingresada\n",
        "CPU\t| numpy.random.randn( Cantidad_N )\t| Inicializa los vectoes X e Y.\n",
        "CPU | numpy.empty_like(y_cpu) | Devuelvo un nuevo vector con el mismo formato que el pasado por parametro\n",
        "**GPU**\t| cuda.mem_alloc() |\tReserva la memoria en GPU.\n",
        "**GPU**\t| cuda.memcpy_htod() | \tCopia las memorias desde el CPU al GPU.\n",
        "CPU\t| SourceModule()\t| Define el código del kernel\n",
        "CPU\t| module.get_function()\t| Genera la función del kernel GPU\n",
        "CPU\t| dim_tx/dim_bx\t| Calcula las dimensiones.\n",
        "**GPU**\t| kernel()\t| Ejecuta el kernel en GPU\n",
        "CPU\t| cuda.memcpy_dtoh( )\t|  Copia el resultado desde GPU memoria Y a CPU memoria R.\n",
        "CPU\t|print()\t| Informo el vector resultante, cantidad de elementos, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvjdxYOKdGQB"
      },
      "source": [
        "**5. Conclusión**\n",
        "\n",
        "Usando los mismos valores que en la versión CPU del ejercicio, se puede observar una clara mejora en los tiempos a niveles grandes de procesamiento. Recordando que en CPU utilizando 50.000 elemento duró un tiempo de 40,532 [ms], vemos que en GPU el tiempo es de 8,566 [ms]. Practicamente una 5ta parte del tiempo usando CPU. Y de ese tiempo, apenas 0,058 [ms] corresponden a GPU. \n",
        "\n",
        "Ahora bien, cuando la cantidad de elementos utilizada como prueba fue 10 ocurrió que el tiempo total del programa, fue mayor en GPU.\n",
        "\n",
        "Tiempo Total CPU: 0,387 [ms]\n",
        "Tiempo Total GPU: 2,087 [ms] - Tiempo GPU: 0,14 [ms]\n",
        "\n",
        "Por lo tanto, se puede llegar a la conclusión de que es recomendable utilizar GPU siempre y cuando la cantidad de procesamiento sea grande (basandose en pruebas realizadas, la diferencia a favor del uso del GPU es a partir de 10.000 elementos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybZpH1KTdH7W"
      },
      "source": [
        "**6. Bibliografía**\n",
        "\n",
        "[1] Introducción a Python [Link](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/Python_Basico.ipynb)\n",
        "\n",
        "[2] Introducción a Colab [Link](https://www.youtube.com/watch?v=ICJP_ukNSQ0)\n",
        "\n",
        "[3] Documentación PYCUDA [Link](https://documen.tician.de/pycuda/)\n",
        "\n",
        "[4] Algoritmo dot [Link](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/blas-and-sparse-blas-routines/blas-routines/blas-level-1-routines-and-functions/cblas-dot.html)\n"
      ]
    }
  ]
}