{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parra_Martin_Ejercicio_2_GPU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYXo7tmxh454"
      },
      "source": [
        "**1. Introducción**\n",
        "\n",
        "El siguiente ejemplo realiza la multiplicación entre un numero (pasado por parámetro) y una matriz[4] utilizando GPGPU. Además, se pasa por parametro el tamaño de las filas y columnas que debe tener la matriz.\n",
        "\n",
        "Utilizando lenguaje Python[1] en Google Colab[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ySb8leyiXHu"
      },
      "source": [
        "**2. Armado del Ambiente**\n",
        "\n",
        "Instala en el cuaderno el módulo CUDA de Python. [3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNB16cdnKQMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b6b778-8945-431d-c532-872bce0ef0fb"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 5.7MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=620899 sha256=6ed11cfa335cc911037a17bc59922dec9c433c46a3ea4360bb72d9cc2858fde6\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=303ac6432fbddf32f3011752be6211726ce7e96b6c1d4a9e3bbe230ef53cc50b\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GamaK9lzpNv7"
      },
      "source": [
        "**3. Desarrollo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNPUz2kxKaf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1e7c89-6595-4f64-c631-293d2942978c"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "TamX =   3#@param {type: \"number\"}\n",
        "TamY =   4#@param {type: \"number\"}\n",
        "Escalar = 5000#@param {type: \"number\"}\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from   pycuda.compiler import SourceModule\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import numpy\n",
        "\n",
        "try:\n",
        "  if TamX <= 0 or TamY <= 0 or Escalar <= 0:\n",
        "    print(\"Ingrese los parametros correctamente. (TamX, TamY y Escalar deben ser mayores a 0\")\n",
        "  else:\n",
        "    # --------------------------------------------\n",
        "    # Definición de función que transforma el tiempo en  milisegundos \n",
        "    tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "    # CPU - Defino la memoria de las matrices en cpu.\n",
        "    matriz = numpy.random.random((TamX,TamY))\n",
        "    matriz = matriz.astype(numpy.float32())\n",
        "\n",
        "    matrizResultado = numpy.random.random((TamX,TamY))\n",
        "    matrizResultado = matriz.astype(numpy.float32())\n",
        "\n",
        "    # CPU - reservo la memoria GPU.\n",
        "    matrizGPU = cuda.mem_alloc(matriz.nbytes)\n",
        "    matrizResultadoGPU = cuda.mem_alloc(matrizResultado.nbytes)\n",
        "\n",
        "    # GPU - Copio la memoria al GPU.\n",
        "    cuda.memcpy_htod(matrizGPU,matriz)\n",
        "    cuda.memcpy_htod(matrizResultadoGPU,matrizResultado)\n",
        "\n",
        "    # CPU - Informo el resutlado.\n",
        "    print( \"------------------------------------\")\n",
        "    print( \"Matriz Original: \" )\n",
        "    print( matriz )\n",
        "    print( \"------------------------------------\")\n",
        "    #CPU - Defino la funcion kernel que ejecutará en GPU\n",
        "    module = SourceModule(\"\"\"\n",
        "    __global__ void kernel_matriz(int filas, int columnas, float *matriz ,float *matrizResultado, int Escalar)\n",
        "    {\n",
        "        int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "        int idy = threadIdx.y + blockIdx.y*blockDim.y;\n",
        "\n",
        "        if(idx<filas && idy<columnas)\n",
        "        {\n",
        "          matrizResultado[idy*filas + idx] = Escalar * matriz[idx*columnas + idy];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    kernel = module.get_function(\"kernel_matriz\")\n",
        "\n",
        "    dim_hilo_x = 16\n",
        "    dim_bloque_x = numpy.int( (TamX+dim_hilo_x-1) / dim_hilo_x )\n",
        "\n",
        "    dim_hilo_y = 16\n",
        "    dim_bloque_y = numpy.int( (TamY+dim_hilo_y-1) / dim_hilo_y )\n",
        "\n",
        "    print( \"Thread x: \", dim_hilo_x, \", Bloque x:\", dim_bloque_x )\n",
        "    print( \"Thread y: \", dim_hilo_y, \", Bloque y:\", dim_bloque_y )\n",
        "\n",
        "    tiempo_kernel = datetime.now()\n",
        "\n",
        "\n",
        "    kernel( numpy.int32(TamX), numpy.int32(TamY), matrizGPU, matrizResultadoGPU, numpy.int32(Escalar), block=( dim_hilo_x, dim_hilo_y, 1 ),grid=(dim_bloque_x, dim_hilo_y,1) )\n",
        "\n",
        "    tiempo_kernel = datetime.now() - tiempo_kernel\n",
        "\n",
        "    cuda.memcpy_dtoh(matrizResultado,matrizResultadoGPU)\n",
        "\n",
        "    tiempo_total = datetime.now() - tiempo_total\n",
        "    print( \"------------------------------------\")\n",
        "    print(\"Matriz Resultado: \")\n",
        "    print(matrizResultado)\n",
        "    print( \"------------------------------------\")\n",
        "    print(\"Tiempo TOTAL: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "    print(\"Tiempo GPU  : \", tiempo_en_ms( tiempo_kernel ), \"[ms]\" )\n",
        "except Exception as exception:\n",
        "  print(\"Ha ocurrido una excepcion: \", exception)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Matriz Original: \n",
            "[[0.09634965 0.37397656 0.52419794 0.77885205]\n",
            " [0.91160655 0.7586633  0.9115434  0.5276657 ]\n",
            " [0.56648964 0.35144705 0.99532706 0.2814266 ]]\n",
            "------------------------------------\n",
            "Thread x:  16 , Bloque x: 1\n",
            "Thread y:  16 , Bloque y: 1\n",
            "------------------------------------\n",
            "Matriz Resultado: \n",
            "[[ 481.74826 4558.0327  2832.4482  1869.8828 ]\n",
            " [3793.3164  1757.2352  2620.9897  4557.7173 ]\n",
            " [4976.6353  3894.2603  2638.3284  1407.133  ]]\n",
            "------------------------------------\n",
            "Tiempo TOTAL:  5.216 [ms]\n",
            "Tiempo GPU  :  0.12 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWqKp6rAiwyD"
      },
      "source": [
        "**4. Tabla de Pasos**\n",
        "\n",
        "Procesador\t| Función\t| Detalle\n",
        "------------|---------|--------\n",
        "CPU\t| @param\t| Lectura del tamaño de vectores desde Colab.\n",
        "CPU\t| import\t| Importa los módulos para funcionar.\n",
        "CPU\t| datetime.now()\t| Toma el tiempo actual.\n",
        "CPU | if | Verifico los parametros\n",
        "CPU\t| numpy.random.random((TamX,TamY))\t| Inicializa las matrices.\n",
        "**GPU**\t| cuda.mem_alloc()\t| Reserva la memoria en GPU.\n",
        "**GPU**\t| cuda.memcpy_htod()\t| Copia las memorias desde el CPU al GPU.\n",
        "CPU\t| SourceModule()\t| Define el código del kernel\n",
        "CPU\t| module.get_function()\t| Genera la función del kernel GPU\n",
        "CPU\t| dim_hilo_x, dim_hilo_y\t| Calculo las dimensiones para la ejecucion en 2D\n",
        "**GPU**\t| kernel()\t| Ejecuta el kernel en GPU\n",
        "CPU\t| cuda.memcpy_dtoh( )\t| Copia el resultado desde GPU memoria Y a CPU memoria R.\n",
        "CPU\t| print()\t| Informo la matriz resultante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWcFpzKSizJt"
      },
      "source": [
        "**5. Conclusión**\n",
        "\n",
        "Continuando con las pruebas realizadas en CPU, replique lo mismo en GPU.\n",
        "\n",
        "Ante una matriz 500x500 los valores en general bajan mucho\n",
        "\n",
        "Tiempo TOTAL:  14,27 [ms] - Tiempo GPU  :  0,094 [ms]\n",
        "\n",
        "Recordando que al ejecutarse en CPU, el tiempo TOTAL de ejecución fue mucho mayor Tiempo Total: 820,462 [ms].\n",
        "\n",
        "En estos casos se puede ver que ante una matriz de tamaño grande, es recomendable utilizarla con GPU.\n",
        "\n",
        "En cuanto a la matriz de 3x4 los valores son los opuestos, a la conclusión que se llega en este caso es la recomendación de ejecutar en CPU si la matriz es pequeña.\n",
        "\n",
        "Tiempo TOTAL:  5,216 [ms] - Tiempo GPU  :  0,12 [ms]\n",
        "\n",
        "Tiempo Total en CPU: 3,841 [ms]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZD6WucBi0pm"
      },
      "source": [
        "**6. Bibliografía**\n",
        "\n",
        "[1] Introducción a Python [Link](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/Python_Basico.ipynb)\n",
        "\n",
        "[2] Introducción a Colab [Link](https://www.youtube.com/watch?v=ICJP_ukNSQ0)\n",
        "\n",
        "[3] Documentación PYCUDA [Link](https://documen.tician.de/pycuda/)\n",
        "\n",
        "[4] Algoritmo Matriz * Escalar [Link](https://es.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:multiplying-matrices-by-scalars/a/multiplying-matrices-by-scalars)"
      ]
    }
  ]
}